# This code belongs to the paper
#
# F. AltekrÃ¼ger and J. Hertrich. 
# WPPNets and WPPFlows: The Power of Wasserstein Patch Priors for Superresolution. 
# ArXiv Preprint#2201.08157
#
# Please cite the paper, if you use the code.
#
# The script reproduces the numerical example with the material 'SiC Diamonds - WPPFlow'
# in the paper.

import torch
from torch import nn
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('agg')
import numpy as np
import os
import model.wppflow as flow
import random
import utils
import argparse
from tqdm import tqdm
import torch.nn.functional as F

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(DEVICE)

def Downsample(scale = 0.25, gaussian_std = 2):
    ''' 
    downsamples an img by factor 4 using gaussian downsample from utils.py
    '''
    if scale > 1:
        print('Error. Scale factor is larger than 1.')
        return
    gaussian_std = gaussian_std
    kernel_size = 16
    gaussian_down = utils.gaussian_downsample(kernel_size,gaussian_std,int(1/scale),pad=True) #gaussian downsample with zero padding
    return gaussian_down.to(DEVICE)

def mean_img(input_img,number):
    '''
    Computes mean and pixelwise standard deviation of 'number' img 
    generated by our WPPFlow
    '''
    mean_img = 0
    shape = [1,1,input_img.shape[2]*scale_factor,input_img.shape[3]*scale_factor]
    for i in range(number):
        z = torch.randn(shape, device = DEVICE)
        with torch.no_grad():
            pred, jac = net(z,input_img)
        utils.save_img(pred, 'results/WPPFlow_predictions/x'+str(scale_factor)+'/Pred' + str(i))
    for i in range(number):
        pred = utils.imread('results/WPPFlow_predictions/x'+str(scale_factor)+'/Pred' + str(i)+'.png')
        mean_img += pred
    mean_img = mean_img / number #mean image 
    
    std_deviation = 0
    for i in range(number):
        pred = utils.imread('results/WPPFlow_predictions/x'+str(scale_factor)+'/Pred' + str(i)+'.png')
        std_deviation += (pred - mean_img)**2
    std_deviation=torch.sqrt(std_deviation/number)
    return [mean_img,std_deviation]

def WLoss(args, input_img, ref_pat, model, psi):
    '''
    Computes the proposed wasserstein loss fct consisting of a MSELoss and a Wasserstein regularizer
    '''
    lam = args.lam
    n_patches_out = args.n_patches_out
    patch_size = args.patch_size
    n_iter_psi = args.n_iter_psi
    keops = args.keops
    
    im2patch = utils.patch_extractor(patch_size,center=args.center)
    
    num_ref = ref_pat.shape[0] #number of patches of reference image
    patch_weights = torch.ones(num_ref,device=DEVICE,dtype=torch.float) #same weight for all patches
    
    semidual_loss = utils.semidual(ref_pat,usekeops=keops) 
    semidual_loss.psi.data = psi #update the maximizer psi from previous step
    
    #sample from normal distribution
    z = torch.randn([input_img.shape[0],1,hr_size,hr_size],device=DEVICE)
    
    pred, log_jac = model(z, input_img) #x4 of input_img
    
    #sum up all patches of whole batch
    inp_pat = torch.empty(0, device = DEVICE)
    for k in range(pred.shape[0]):
        inp = im2patch(pred[k,:,:,:].unsqueeze(0)) #use all patches of input_img
        inp_pat = torch.cat([inp_pat,inp],0)
    inp = inp_pat

    #gradient ascent to find maximizer psi for dual formulation of W2^2
    optim_psi = torch.optim.ASGD([semidual_loss.psi], lr=1e-0, alpha=0.5, t0=1)
    for i in range(n_iter_psi):
        sem = -semidual_loss(inp,patch_weights)
        optim_psi.zero_grad()
        sem.backward(retain_graph=True)
        optim_psi.step()
    semidual_loss.psi.data = optim_psi.state[semidual_loss.psi]['ax']
    psi = semidual_loss.psi.data #update psi
    
    reg = semidual_loss(inp,patch_weights) #wasserstein regularizer 

    down_pred = operator(pred) #downsample pred by scale_factor

    l2 = torch.sum((down_pred - input_img)**2) / input_img.shape[0] #||f(T(z,y)) - y||^2
    
    total_loss = l2/2 + lam * reg - (std**2)*torch.mean(log_jac)
    return [total_loss,l2/2,lam*reg,psi]

def training(trainset, model, reference_img, batch_size, epochs, args, opti):
    '''
    training process
    '''
    numb_train_img = trainset.shape[0] #number of all img
    
    #create random batches:
    idx = torch.randperm(numb_train_img)
    batch_lr = [] #list of batches
    for i in range(0,numb_train_img,batch_size):
        batch_lr.append(trainset[i:(i+batch_size),...])
    
    #create maximizer psi
    psi_length = args.n_patches_out #length of vector psi
    psi_list = []
    for i in range(len(batch_lr)):
        psi_list.append(torch.zeros(psi_length, device = DEVICE)) #create a list consisting of psi

    #create random patches of reference image
    im2patch = utils.patch_extractor(args.patch_size,center=args.center)
    ref = im2patch(reference_img,args.n_patches_out)
    
    a_psnr_list = [] #for validation
    loss_list = []; reg_list = []; MSE_list = [] #for plot

    for t in tqdm(range(epochs)):
        a_totalloss = 0; a_MSE = 0; a_reg = 0
        ints = random.sample(range(0,len(batch_lr)),len(batch_lr)) #random order of batches
        for i in tqdm(ints):
            psi_temp = psi_list[i] #choose corresponding saved maximizer psi  
            [total_loss,loss,reg,p] = WLoss(args, batch_lr[i], ref, model, psi_temp)  
    
            #backpropagation
            opti.zero_grad()
            total_loss.backward()
            #gradient clipping to prevent an unstable behaviour
            nn.utils.clip_grad_norm_(model.parameters(), 1)
            opti.step()
            
            total_loss = total_loss.item(); loss = loss.item(); reg = reg.item()
            a_totalloss += total_loss; a_MSE += loss; a_reg += reg
            psi_list[i] = p #update psi

        a_totalloss = a_totalloss/len(batch_lr); a_MSE = a_MSE/len(batch_lr); a_reg = a_reg/len(batch_lr)
        loss_list.append(a_totalloss); MSE_list.append(a_MSE); reg_list.append(a_reg)
        
        if not os.path.isdir('checkpoints'):
            os.mkdir('checkpoints')
        
        val_step = 10
        if (t+1)%val_step == 0:
            print(f'------------------------------- \nValidation step')
            val_len = len(args.val)
            a_psnr = 0
            for i in range(val_len):
                with torch.no_grad():
                    z = torch.randn(args.val[i][1].shape,device=DEVICE)
                    pred, _ = net(z, args.val[i][0])
                psnr_val = utils.psnr(pred,args.val[i][1],40)
                a_psnr += psnr_val
            a_psnr = a_psnr / val_len
            print(f'Average Validation PSNR: {a_psnr}')    
            a_psnr_list.append(a_psnr)
            plt.plot(list(range(val_step,val_step*len(a_psnr_list)+val_step,val_step)),a_psnr_list, 'k')
            title = 'Avarage PSNR ' + str(round(a_psnr,2))
            plt.title(title)
            plt.savefig('checkpoints/ValidatonPSNR_WPPFlow_x'+str(scale_factor)+'.pdf')
            plt.close()
            print(f'-------------------------------')
        
        #save a checkpoint
        if (t+1)%30 == 0:
            torch.save({'net_state_dict': model.state_dict()}, 'checkpoints/checkpoint_WPPFlow_x'+str(scale_factor) +'.pth')
            with torch.no_grad():
                z = torch.randn(hr.shape,device=DEVICE)
                pred_hr, _  = model(z, lr)
            if not os.path.isdir('checkpoints/tmp'):
                os.mkdir('checkpoints/tmp')
            utils.save_img(pred_hr,'checkpoints/tmp/pred'+str(t+1))
            plt.ylabel('Loss')
            plt.xlabel('Epoch')
            plt.plot(list(range(len(loss_list))), loss_list, 'k-.', label='avarage loss')
            plt.plot(list(range(len(MSE_list))), MSE_list, 'k-', label='avarage MSE')
            plt.plot(list(range(len(reg_list))), reg_list, 'k:', label='avarage Reg')
            plt.legend(loc='upper right')
            plt.yscale('log')
            plt.savefig('checkpoints/losscurve_WPPFlow_x'+str(scale_factor)+'.pdf')
            plt.close()

retrain = False
if __name__ == '__main__':
    if not os.path.isdir('results'):
       os.mkdir('results')    
    image_class = 'SiC'
    scale_factor = 4 #choose between scale factor 4 and 8
    trainset_size = 1000
    if scale_factor == 8:
        trainset_size = 270
    lr_train = utils.Trainset(image_class = 'SiC_x'+str(scale_factor), size = trainset_size)
    std = 0.01
    
    hr_size = lr_train.shape[2] * scale_factor
    net = flow.WPPFlow(scale=scale_factor, hr_size = hr_size).to(device=DEVICE)
    print('Superresolution WPPFlow with scale factor ' + str(scale_factor))
    
    hr = utils.imread('test_img/hr_SiC.png')
    lr = utils.imread('test_img/lr_SiC_x' + str(scale_factor) + '.png')
    #  = operator(hr) + 0.01*torch.randn_like(operator(hr))
    if retrain:
        #inputs
        operator = Downsample(scale = 1/scale_factor, gaussian_std = int(scale_factor/2))   

        val = utils.Validationset(image_class = image_class+'_x'+str(scale_factor))	

        args=argparse.Namespace()
        args.lam=100
        args.n_patches_out=10000
        args.patch_size=6
        args.n_iter_psi=10
        args.val = val
        args.keops = True
        args.center = True

        reference_img = utils.imread('test_img/ref_SiC.png')        
        
        #training process
        batch_size = 10
        epochs = 450
        
        learning_rate = 2e-4
        OPTIMIZER = torch.optim.Adam(net.parameters(), lr=learning_rate)    
        
        training(lr_train,net,reference_img,batch_size,epochs,args=args,opti=OPTIMIZER)
        with torch.no_grad():
            z = torch.randn(hr.shape,device=DEVICE)
            pred, _  = net(z, lr)
        torch.save({'net_state_dict': net.state_dict(), 'optimizer_state_dict': OPTIMIZER.state_dict()},
                    'results/weights_WPPFlow_x'+str(scale_factor)+'.pth')        
        utils.save_img(pred,'results/W2_WPPFlow_x'+str(scale_factor))
            
    if not retrain:
        weights = torch.load('results/weights_WPPFlow_x'+str(scale_factor)+'.pth')
        net.load_state_dict(weights['net_state_dict'])
        z = torch.randn(hr.shape,device=DEVICE)
        with torch.no_grad():
            pred, _  = net(z, lr)
        utils.save_img(pred,'results/W2_WPPFlow_x'+str(scale_factor))

